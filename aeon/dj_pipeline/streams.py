#----                     DO NOT MODIFY                ----
#---- THIS FILE IS AUTO-GENERATED BY `streams_maker.py` ----

import datajoint as dj
import pandas as pd
from uuid import UUID

import aeon
from aeon.dj_pipeline import acquisition, get_schema_name
from aeon.io import api as io_api

schema = dj.Schema(get_schema_name("streams"))


@schema 
class StreamType(dj.Lookup):
    """
    Catalog of all steam types for the different device types used across Project Aeon
    One StreamType corresponds to one reader class in `aeon.io.reader`
    The combination of `stream_reader` and `stream_reader_kwargs` should fully specify
    the data loading routine for a particular device, using the `aeon.io.utils`
    """

    definition = """  # Catalog of all stream types used across Project Aeon
    stream_type          : varchar(20)
    ---
    stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
    stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
    stream_description='': varchar(256)
    stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
    unique index (stream_hash)
    """


@schema 
class DeviceType(dj.Lookup):
    """
    Catalog of all device types used across Project Aeon
    """

    definition = """  # Catalog of all device types used across Project Aeon
    device_type:             varchar(36)
    ---
    device_description='':   varchar(256)
    """

    class Stream(dj.Part):
        definition = """  # Data stream(s) associated with a particular device type
        -> master
        -> StreamType
        """


@schema 
class Device(dj.Lookup):
    definition = """  # Physical devices, of a particular type, identified by unique serial number
    device_serial_number: varchar(12)
    ---
    -> DeviceType
    """


@schema 
class UndergroundFeeder(dj.Manual):
        definition = f"""
        # underground_feeder placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-unknown)
        -> acquisition.Experiment
        -> Device
        underground_feeder_install_time  : datetime(6)   # time of the underground_feeder placed and started operation at this position
        ---
        underground_feeder_name          : varchar(36)
        """

        class Attribute(dj.Part):
            definition = """  # metadata/attributes (e.g. FPS, config, calibration, etc.) associated with this experimental device
            -> master
            attribute_name          : varchar(32)
            ---
            attribute_value=null    : longblob
            """

        class RemovalTime(dj.Part):
            definition = f"""
            -> master
            ---
            underground_feeder_removal_time: datetime(6)  # time of the underground_feeder being removed
            """


@schema 
class VideoSource(dj.Manual):
        definition = f"""
        # video_source placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-unknown)
        -> acquisition.Experiment
        -> Device
        video_source_install_time  : datetime(6)   # time of the video_source placed and started operation at this position
        ---
        video_source_name          : varchar(36)
        """

        class Attribute(dj.Part):
            definition = """  # metadata/attributes (e.g. FPS, config, calibration, etc.) associated with this experimental device
            -> master
            attribute_name          : varchar(32)
            ---
            attribute_value=null    : longblob
            """

        class RemovalTime(dj.Part):
            definition = f"""
            -> master
            ---
            video_source_removal_time: datetime(6)  # time of the video_source being removed
            """


@schema 
class UndergroundFeederBeamBreak(dj.Imported):
        definition = """  # Raw per-chunk BeamBreak data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
    -> UndergroundFeeder
    -> acquisition.Chunk
    ---
    sample_count: int      # number of data points acquired from this stream for a given chunk
    timestamps: longblob   # (datetime) timestamps of BeamBreak data
    event: longblob
    """
        _stream_reader = aeon.io.reader.BitmaskEvent
        _stream_detail = {'stream_type': 'BeamBreak', 'stream_reader': 'aeon.io.reader.BitmaskEvent', 'stream_reader_kwargs': {'pattern': '{pattern}_32', 'value': 34, 'tag': 'BeamBroken'}, 'stream_description': '', 'stream_hash': UUID('b14171e6-d27d-117a-ae73-a16c4b5fc8a2')}

        @property
        def key_source(self):
            f"""
            Only the combination of Chunk and UndergroundFeeder with overlapping time
            +  Chunk(s) that started after UndergroundFeeder install time and ended before UndergroundFeeder remove time
            +  Chunk(s) that started after UndergroundFeeder install time for UndergroundFeeder that are not yet removed
            """
            return (
                acquisition.Chunk
                * UndergroundFeeder.join(UndergroundFeeder.RemovalTime, left=True)
                & 'chunk_start >= underground_feeder_install_time'
                & 'chunk_start < IFNULL(underground_feeder_removal_time, "2200-01-01")'
            )

        def make(self, key):
            chunk_start, chunk_end, dir_type = (acquisition.Chunk & key).fetch1(
                "chunk_start", "chunk_end", "directory_type"
            )
            raw_data_dir = acquisition.Experiment.get_data_directory(
                key, directory_type=dir_type
            )

            device_name = (UndergroundFeeder & key).fetch1(
                'underground_feeder_name'
            )

            stream = self._stream_reader(
                **{
                    k: v.format(**{k: device_name}) if k == "pattern" else v
                    for k, v in self._stream_detail["stream_reader_kwargs"].items()
                }
            )

            stream_data = io_api.load(
                root=raw_data_dir.as_posix(),
                reader=stream,
                start=pd.Timestamp(chunk_start),
                end=pd.Timestamp(chunk_end),
            )

            self.insert1(
                {
                    **key,
                    "sample_count": len(stream_data),
                    "timestamps": stream_data.index.values,
                    **{
                        c: stream_data[c].values
                        for c in stream.columns
                        if not c.startswith("_")
                    },
                }, ignore_extra_fields=True
            )


@schema 
class UndergroundFeederDeliverPellet(dj.Imported):
        definition = """  # Raw per-chunk DeliverPellet data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
    -> UndergroundFeeder
    -> acquisition.Chunk
    ---
    sample_count: int      # number of data points acquired from this stream for a given chunk
    timestamps: longblob   # (datetime) timestamps of DeliverPellet data
    event: longblob
    """
        _stream_reader = aeon.io.reader.BitmaskEvent
        _stream_detail = {'stream_type': 'DeliverPellet', 'stream_reader': 'aeon.io.reader.BitmaskEvent', 'stream_reader_kwargs': {'pattern': '{pattern}_35', 'value': 1, 'tag': 'TriggeredPellet'}, 'stream_description': '', 'stream_hash': UUID('c49dda51-2e38-8b49-d1d8-2e54ea928e9c')}

        @property
        def key_source(self):
            f"""
            Only the combination of Chunk and UndergroundFeeder with overlapping time
            +  Chunk(s) that started after UndergroundFeeder install time and ended before UndergroundFeeder remove time
            +  Chunk(s) that started after UndergroundFeeder install time for UndergroundFeeder that are not yet removed
            """
            return (
                acquisition.Chunk
                * UndergroundFeeder.join(UndergroundFeeder.RemovalTime, left=True)
                & 'chunk_start >= underground_feeder_install_time'
                & 'chunk_start < IFNULL(underground_feeder_removal_time, "2200-01-01")'
            )

        def make(self, key):
            chunk_start, chunk_end, dir_type = (acquisition.Chunk & key).fetch1(
                "chunk_start", "chunk_end", "directory_type"
            )
            raw_data_dir = acquisition.Experiment.get_data_directory(
                key, directory_type=dir_type
            )

            device_name = (UndergroundFeeder & key).fetch1(
                'underground_feeder_name'
            )

            stream = self._stream_reader(
                **{
                    k: v.format(**{k: device_name}) if k == "pattern" else v
                    for k, v in self._stream_detail["stream_reader_kwargs"].items()
                }
            )

            stream_data = io_api.load(
                root=raw_data_dir.as_posix(),
                reader=stream,
                start=pd.Timestamp(chunk_start),
                end=pd.Timestamp(chunk_end),
            )

            self.insert1(
                {
                    **key,
                    "sample_count": len(stream_data),
                    "timestamps": stream_data.index.values,
                    **{
                        c: stream_data[c].values
                        for c in stream.columns
                        if not c.startswith("_")
                    },
                }, ignore_extra_fields=True
            )


@schema 
class UndergroundFeederDepletionState(dj.Imported):
        definition = """  # Raw per-chunk DepletionState data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
    -> UndergroundFeeder
    -> acquisition.Chunk
    ---
    sample_count: int      # number of data points acquired from this stream for a given chunk
    timestamps: longblob   # (datetime) timestamps of DepletionState data
    threshold: longblob
    d1: longblob
    delta: longblob
    """
        _stream_reader = aeon.schema.foraging._PatchState
        _stream_detail = {'stream_type': 'DepletionState', 'stream_reader': 'aeon.schema.foraging._PatchState', 'stream_reader_kwargs': {'pattern': '{pattern}_State_*'}, 'stream_description': '', 'stream_hash': UUID('17c3e36f-3f2e-2494-bbd3-5cb9a23d3039')}

        @property
        def key_source(self):
            f"""
            Only the combination of Chunk and UndergroundFeeder with overlapping time
            +  Chunk(s) that started after UndergroundFeeder install time and ended before UndergroundFeeder remove time
            +  Chunk(s) that started after UndergroundFeeder install time for UndergroundFeeder that are not yet removed
            """
            return (
                acquisition.Chunk
                * UndergroundFeeder.join(UndergroundFeeder.RemovalTime, left=True)
                & 'chunk_start >= underground_feeder_install_time'
                & 'chunk_start < IFNULL(underground_feeder_removal_time, "2200-01-01")'
            )

        def make(self, key):
            chunk_start, chunk_end, dir_type = (acquisition.Chunk & key).fetch1(
                "chunk_start", "chunk_end", "directory_type"
            )
            raw_data_dir = acquisition.Experiment.get_data_directory(
                key, directory_type=dir_type
            )

            device_name = (UndergroundFeeder & key).fetch1(
                'underground_feeder_name'
            )

            stream = self._stream_reader(
                **{
                    k: v.format(**{k: device_name}) if k == "pattern" else v
                    for k, v in self._stream_detail["stream_reader_kwargs"].items()
                }
            )

            stream_data = io_api.load(
                root=raw_data_dir.as_posix(),
                reader=stream,
                start=pd.Timestamp(chunk_start),
                end=pd.Timestamp(chunk_end),
            )

            self.insert1(
                {
                    **key,
                    "sample_count": len(stream_data),
                    "timestamps": stream_data.index.values,
                    **{
                        c: stream_data[c].values
                        for c in stream.columns
                        if not c.startswith("_")
                    },
                }, ignore_extra_fields=True
            )


@schema 
class UndergroundFeederEncoder(dj.Imported):
        definition = """  # Raw per-chunk Encoder data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
    -> UndergroundFeeder
    -> acquisition.Chunk
    ---
    sample_count: int      # number of data points acquired from this stream for a given chunk
    timestamps: longblob   # (datetime) timestamps of Encoder data
    angle: longblob
    intensity: longblob
    """
        _stream_reader = aeon.io.reader.Encoder
        _stream_detail = {'stream_type': 'Encoder', 'stream_reader': 'aeon.io.reader.Encoder', 'stream_reader_kwargs': {'pattern': '{pattern}_90_*'}, 'stream_description': '', 'stream_hash': UUID('f96b0b26-26f6-5ff6-b3c7-5aa5adc00c1a')}

        @property
        def key_source(self):
            f"""
            Only the combination of Chunk and UndergroundFeeder with overlapping time
            +  Chunk(s) that started after UndergroundFeeder install time and ended before UndergroundFeeder remove time
            +  Chunk(s) that started after UndergroundFeeder install time for UndergroundFeeder that are not yet removed
            """
            return (
                acquisition.Chunk
                * UndergroundFeeder.join(UndergroundFeeder.RemovalTime, left=True)
                & 'chunk_start >= underground_feeder_install_time'
                & 'chunk_start < IFNULL(underground_feeder_removal_time, "2200-01-01")'
            )

        def make(self, key):
            chunk_start, chunk_end, dir_type = (acquisition.Chunk & key).fetch1(
                "chunk_start", "chunk_end", "directory_type"
            )
            raw_data_dir = acquisition.Experiment.get_data_directory(
                key, directory_type=dir_type
            )

            device_name = (UndergroundFeeder & key).fetch1(
                'underground_feeder_name'
            )

            stream = self._stream_reader(
                **{
                    k: v.format(**{k: device_name}) if k == "pattern" else v
                    for k, v in self._stream_detail["stream_reader_kwargs"].items()
                }
            )

            stream_data = io_api.load(
                root=raw_data_dir.as_posix(),
                reader=stream,
                start=pd.Timestamp(chunk_start),
                end=pd.Timestamp(chunk_end),
            )

            self.insert1(
                {
                    **key,
                    "sample_count": len(stream_data),
                    "timestamps": stream_data.index.values,
                    **{
                        c: stream_data[c].values
                        for c in stream.columns
                        if not c.startswith("_")
                    },
                }, ignore_extra_fields=True
            )


@schema 
class VideoSourcePosition(dj.Imported):
        definition = """  # Raw per-chunk Position data stream from VideoSource (auto-generated with aeon_mecha-unknown)
    -> VideoSource
    -> acquisition.Chunk
    ---
    sample_count: int      # number of data points acquired from this stream for a given chunk
    timestamps: longblob   # (datetime) timestamps of Position data
    x: longblob
    y: longblob
    angle: longblob
    major: longblob
    minor: longblob
    area: longblob
    id: longblob
    """
        _stream_reader = aeon.io.reader.Position
        _stream_detail = {'stream_type': 'Position', 'stream_reader': 'aeon.io.reader.Position', 'stream_reader_kwargs': {'pattern': '{pattern}_200_*'}, 'stream_description': '', 'stream_hash': UUID('d7727726-1f52-78e1-1355-b863350b6d03')}

        @property
        def key_source(self):
            f"""
            Only the combination of Chunk and VideoSource with overlapping time
            +  Chunk(s) that started after VideoSource install time and ended before VideoSource remove time
            +  Chunk(s) that started after VideoSource install time for VideoSource that are not yet removed
            """
            return (
                acquisition.Chunk
                * VideoSource.join(VideoSource.RemovalTime, left=True)
                & 'chunk_start >= video_source_install_time'
                & 'chunk_start < IFNULL(video_source_removal_time, "2200-01-01")'
            )

        def make(self, key):
            chunk_start, chunk_end, dir_type = (acquisition.Chunk & key).fetch1(
                "chunk_start", "chunk_end", "directory_type"
            )
            raw_data_dir = acquisition.Experiment.get_data_directory(
                key, directory_type=dir_type
            )

            device_name = (VideoSource & key).fetch1(
                'video_source_name'
            )

            stream = self._stream_reader(
                **{
                    k: v.format(**{k: device_name}) if k == "pattern" else v
                    for k, v in self._stream_detail["stream_reader_kwargs"].items()
                }
            )

            stream_data = io_api.load(
                root=raw_data_dir.as_posix(),
                reader=stream,
                start=pd.Timestamp(chunk_start),
                end=pd.Timestamp(chunk_end),
            )

            self.insert1(
                {
                    **key,
                    "sample_count": len(stream_data),
                    "timestamps": stream_data.index.values,
                    **{
                        c: stream_data[c].values
                        for c in stream.columns
                        if not c.startswith("_")
                    },
                }, ignore_extra_fields=True
            )


@schema 
class VideoSourceRegion(dj.Imported):
        definition = """  # Raw per-chunk Region data stream from VideoSource (auto-generated with aeon_mecha-unknown)
    -> VideoSource
    -> acquisition.Chunk
    ---
    sample_count: int      # number of data points acquired from this stream for a given chunk
    timestamps: longblob   # (datetime) timestamps of Region data
    region: longblob
    """
        _stream_reader = aeon.schema.foraging._RegionReader
        _stream_detail = {'stream_type': 'Region', 'stream_reader': 'aeon.schema.foraging._RegionReader', 'stream_reader_kwargs': {'pattern': '{pattern}_201_*'}, 'stream_description': '', 'stream_hash': UUID('6c78b3ac-ffff-e2ab-c446-03e3adf4d80a')}

        @property
        def key_source(self):
            f"""
            Only the combination of Chunk and VideoSource with overlapping time
            +  Chunk(s) that started after VideoSource install time and ended before VideoSource remove time
            +  Chunk(s) that started after VideoSource install time for VideoSource that are not yet removed
            """
            return (
                acquisition.Chunk
                * VideoSource.join(VideoSource.RemovalTime, left=True)
                & 'chunk_start >= video_source_install_time'
                & 'chunk_start < IFNULL(video_source_removal_time, "2200-01-01")'
            )

        def make(self, key):
            chunk_start, chunk_end, dir_type = (acquisition.Chunk & key).fetch1(
                "chunk_start", "chunk_end", "directory_type"
            )
            raw_data_dir = acquisition.Experiment.get_data_directory(
                key, directory_type=dir_type
            )

            device_name = (VideoSource & key).fetch1(
                'video_source_name'
            )

            stream = self._stream_reader(
                **{
                    k: v.format(**{k: device_name}) if k == "pattern" else v
                    for k, v in self._stream_detail["stream_reader_kwargs"].items()
                }
            )

            stream_data = io_api.load(
                root=raw_data_dir.as_posix(),
                reader=stream,
                start=pd.Timestamp(chunk_start),
                end=pd.Timestamp(chunk_end),
            )

            self.insert1(
                {
                    **key,
                    "sample_count": len(stream_data),
                    "timestamps": stream_data.index.values,
                    **{
                        c: stream_data[c].values
                        for c in stream.columns
                        if not c.startswith("_")
                    },
                }, ignore_extra_fields=True
            )


@schema 
class VideoSourceVideo(dj.Imported):
        definition = """  # Raw per-chunk Video data stream from VideoSource (auto-generated with aeon_mecha-unknown)
    -> VideoSource
    -> acquisition.Chunk
    ---
    sample_count: int      # number of data points acquired from this stream for a given chunk
    timestamps: longblob   # (datetime) timestamps of Video data
    hw_counter: longblob
    hw_timestamp: longblob
    """
        _stream_reader = aeon.io.reader.Video
        _stream_detail = {'stream_type': 'Video', 'stream_reader': 'aeon.io.reader.Video', 'stream_reader_kwargs': {'pattern': '{pattern}_*'}, 'stream_description': '', 'stream_hash': UUID('f51c6174-e0c4-a888-3a9d-6f97fb6a019b')}

        @property
        def key_source(self):
            f"""
            Only the combination of Chunk and VideoSource with overlapping time
            +  Chunk(s) that started after VideoSource install time and ended before VideoSource remove time
            +  Chunk(s) that started after VideoSource install time for VideoSource that are not yet removed
            """
            return (
                acquisition.Chunk
                * VideoSource.join(VideoSource.RemovalTime, left=True)
                & 'chunk_start >= video_source_install_time'
                & 'chunk_start < IFNULL(video_source_removal_time, "2200-01-01")'
            )

        def make(self, key):
            chunk_start, chunk_end, dir_type = (acquisition.Chunk & key).fetch1(
                "chunk_start", "chunk_end", "directory_type"
            )
            raw_data_dir = acquisition.Experiment.get_data_directory(
                key, directory_type=dir_type
            )

            device_name = (VideoSource & key).fetch1(
                'video_source_name'
            )

            stream = self._stream_reader(
                **{
                    k: v.format(**{k: device_name}) if k == "pattern" else v
                    for k, v in self._stream_detail["stream_reader_kwargs"].items()
                }
            )

            stream_data = io_api.load(
                root=raw_data_dir.as_posix(),
                reader=stream,
                start=pd.Timestamp(chunk_start),
                end=pd.Timestamp(chunk_end),
            )

            self.insert1(
                {
                    **key,
                    "sample_count": len(stream_data),
                    "timestamps": stream_data.index.values,
                    **{
                        c: stream_data[c].values
                        for c in stream.columns
                        if not c.startswith("_")
                    },
                }, ignore_extra_fields=True
            )


